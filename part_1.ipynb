{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2rpWI3cRGPTX"
      },
      "source": [
        "# phase 1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D0ddSnxuGLAi"
      },
      "source": [
        "## imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gSUC7gxINeOx",
        "outputId": "af0556b9-9f60-44ad-e9b6-018365a96b46"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting torchinfo\n",
            "  Downloading torchinfo-1.8.0-py3-none-any.whl (23 kB)\n",
            "Installing collected packages: torchinfo\n",
            "Successfully installed torchinfo-1.8.0\n"
          ]
        }
      ],
      "source": [
        "!pip install torchinfo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "DRxbohbATDdy"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision\n",
        "# import utils\n",
        "from typing import List\n",
        "from torchvision import transforms, models, datasets\n",
        "import torch\n",
        "from torchinfo import summary\n",
        "from torch.utils.data import DataLoader, Subset, random_split, ConcatDataset\n",
        "import numpy as np\n",
        "import random\n",
        "from os.path import exists\n",
        "import os\n",
        "from typing import List, Dict, Tuple\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix\n",
        "from sklearn.metrics import ConfusionMatrixDisplay\n",
        "import seaborn as sns\n",
        "from sklearn.utils.multiclass import unique_labels\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0UkOP_xTGApf"
      },
      "source": [
        "## GPU state"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vcJYNoSIqHWw",
        "outputId": "1ee2ea91-1712-4882-c91d-daebd9e19daf"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "device(type='cuda', index=0)"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ],
      "source": [
        "use_gpu = True\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() and use_gpu else \"cpu\")\n",
        "# device = torch.device(\"cpu\")\n",
        "device"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0GIL20ZmUfhD",
        "outputId": "93c1c02c-5730-4c52-f68c-643c3bd88cbc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CUDA available:  True\n",
            "Code running on GPU:  False\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "\n",
        "# Check if CUDA (GPU support) is available\n",
        "print(\"CUDA available: \", torch.cuda.is_available())\n",
        "\n",
        "# Check if code is running on GPU\n",
        "print(\"Code running on GPU: \", torch.cuda.is_initialized())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B03Ai5UkTDd0"
      },
      "source": [
        "## Data Loader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "F07yw0wNTDd2"
      },
      "outputs": [],
      "source": [
        "def get_oxford_splits(\n",
        "    batch_size: int,\n",
        "    data_loader_seed: int = 111,\n",
        "    pin_memory: bool = True,\n",
        "    num_workers: int = 2,\n",
        "    ):\n",
        "    K = 5\n",
        "    num_support = 80\n",
        "    num_query = 20\n",
        "\n",
        "    def seed_worker(worker_id):\n",
        "        # worker_seed = torch.initial_seed() % 2 ** 32\n",
        "        np.random.seed(data_loader_seed)\n",
        "        random.seed(data_loader_seed)\n",
        "    g = torch.Generator()\n",
        "    g.manual_seed(data_loader_seed)\n",
        "\n",
        "    support_classes = list(np.arange(num_support))\n",
        "    query_classes = list(np.arange(num_query) + num_support)\n",
        "\n",
        "    img_dim = 64\n",
        "\n",
        "    train_transforms = transforms.Compose([\n",
        "        transforms.Resize((img_dim, img_dim)),\n",
        "        transforms.RandomHorizontalFlip(),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize([0.485, 0.456, 0.406],[0.229, 0.224, 0.225])])\n",
        "    test_transforms = transforms.Compose([\n",
        "        transforms.Resize((img_dim, img_dim)),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize([0.485, 0.456, 0.406],[0.229, 0.224, 0.225])])\n",
        "    validation_transforms = transforms.Compose([\n",
        "        transforms.Resize((img_dim, img_dim)),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize([0.485, 0.456, 0.406],[0.229, 0.224, 0.225])])\n",
        "\n",
        "    data_path = f'/content/data'\n",
        "\n",
        "    train_ds_full = datasets.Flowers102(root=data_path, split=\"train\", download=True, transform=train_transforms)\n",
        "    val_ds_full = datasets.Flowers102(root=data_path, split=\"val\", download=True, transform=validation_transforms)\n",
        "    test_ds_full = datasets.Flowers102(root=data_path, split=\"test\", download=True, transform=test_transforms)\n",
        "\n",
        "    train_indxs_support = torch.where(torch.isin(torch.tensor(train_ds_full._labels), torch.asarray(support_classes)))[0]\n",
        "    val_indxs_support = torch.where(torch.isin(torch.tensor(val_ds_full._labels), torch.asarray(support_classes)))[0]\n",
        "    test_indxs_support = torch.where(torch.isin(torch.tensor(test_ds_full._labels), torch.asarray(support_classes)))[0]\n",
        "\n",
        "    train_ds_subset_support = torch.utils.data.Subset(train_ds_full, train_indxs_support)\n",
        "    val_ds_subset_support = torch.utils.data.Subset(val_ds_full, val_indxs_support)\n",
        "    test_ds_subset_support = torch.utils.data.Subset(test_ds_full, test_indxs_support)\n",
        "\n",
        "    merged_dataset = ConcatDataset([train_ds_subset_support, val_ds_subset_support, test_ds_subset_support])\n",
        "    ### A, B\n",
        "    train_ds_support, test_ds_support = torch.utils.data.random_split(merged_dataset, [0.75, 0.25], generator=torch.Generator().manual_seed(42))\n",
        "    ###\n",
        "\n",
        "    train_indxs_query = torch.where(torch.isin(torch.tensor(train_ds_full._labels), torch.asarray(query_classes)))[0]\n",
        "    N = 10\n",
        "    starting_indices = np.arange(0, len(train_indxs_query), N)\n",
        "    train_indxs_query = np.hstack([train_indxs_query[i:i+K] for i in starting_indices if i + K <= len(train_indxs_query)])\n",
        "    ### C\n",
        "    train_ds_query = torch.utils.data.Subset(train_ds_full, train_indxs_query)\n",
        "    ###\n",
        "\n",
        "    val_indxs_query = torch.where(torch.isin(torch.tensor(val_ds_full._labels), torch.asarray(query_classes)))[0]\n",
        "    test_indxs_query = torch.where(torch.isin(torch.tensor(test_ds_full._labels), torch.asarray(query_classes)))[0]\n",
        "    val_ds_subset_query = torch.utils.data.Subset(val_ds_full, val_indxs_query)\n",
        "    test_ds_subset_query = torch.utils.data.Subset(test_ds_full, test_indxs_query)\n",
        "\n",
        "    test_ds_query_full = ConcatDataset([val_ds_subset_query, test_ds_subset_query])\n",
        "    ### D\n",
        "    _, test_ds_query = torch.utils.data.random_split(test_ds_query_full, [0.7, 0.3], generator=torch.Generator().manual_seed(42))\n",
        "    ###\n",
        "\n",
        "    ### E\n",
        "    test_all = ConcatDataset([test_ds_query, test_ds_support])\n",
        "\n",
        "\n",
        "    A_train_dl = DataLoader(\n",
        "        train_ds_support,\n",
        "        batch_size = batch_size,\n",
        "        shuffle=True,\n",
        "        worker_init_fn=seed_worker,\n",
        "        generator=g,\n",
        "        drop_last=False,\n",
        "        pin_memory=pin_memory,\n",
        "        num_workers=num_workers\n",
        "    )\n",
        "    A_test_dl = DataLoader(\n",
        "        test_ds_support,\n",
        "        batch_size = batch_size,\n",
        "        shuffle=True,\n",
        "        worker_init_fn=seed_worker,\n",
        "        generator=g,\n",
        "        drop_last=False,\n",
        "        pin_memory=pin_memory,\n",
        "        num_workers=num_workers\n",
        "    )\n",
        "\n",
        "    B_train_dl = DataLoader(\n",
        "        train_ds_query,\n",
        "        batch_size = batch_size,\n",
        "        shuffle=True,\n",
        "        worker_init_fn=seed_worker,\n",
        "        generator=g,\n",
        "        drop_last=False,\n",
        "        pin_memory=pin_memory,\n",
        "        num_workers=num_workers\n",
        "    )\n",
        "    B_test_dl = DataLoader(\n",
        "        test_ds_query,\n",
        "        batch_size = batch_size,\n",
        "        shuffle=True,\n",
        "        worker_init_fn=seed_worker,\n",
        "        generator=g,\n",
        "        drop_last=False,\n",
        "        pin_memory=pin_memory,\n",
        "        num_workers=num_workers\n",
        "    )\n",
        "    test_all = DataLoader(\n",
        "        test_all,\n",
        "        batch_size = batch_size,\n",
        "        shuffle=True,\n",
        "        worker_init_fn=seed_worker,\n",
        "        generator=g,\n",
        "        drop_last=False,\n",
        "        pin_memory=pin_memory,\n",
        "        num_workers=num_workers\n",
        "    )\n",
        "\n",
        "    return A_train_dl, A_test_dl, B_train_dl, B_test_dl, test_all"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6VoGLQNV0AOX"
      },
      "source": [
        "## Split data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nqwdU4e3TDd4",
        "outputId": "a10ddf04-087c-404e-b7e0-7361d08ab426"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading https://thor.robots.ox.ac.uk/datasets/flowers-102/102flowers.tgz to /content/data/flowers-102/102flowers.tgz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 344862509/344862509 [00:12<00:00, 27803245.99it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting /content/data/flowers-102/102flowers.tgz to /content/data/flowers-102\n",
            "Downloading https://thor.robots.ox.ac.uk/datasets/flowers-102/imagelabels.mat to /content/data/flowers-102/imagelabels.mat\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 502/502 [00:00<00:00, 1347977.34it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading https://thor.robots.ox.ac.uk/datasets/flowers-102/setid.mat to /content/data/flowers-102/setid.mat\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 14989/14989 [00:00<00:00, 16099468.03it/s]\n"
          ]
        }
      ],
      "source": [
        "A_train_dl, A_test_dl, B_train_dl, B_test_dl, test_all = get_oxford_splits(\n",
        "    batch_size=250,\n",
        "    data_loader_seed=111,\n",
        "    pin_memory=False,\n",
        "    num_workers=1\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ryLMC8ZYnkou"
      },
      "source": [
        "## plot output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "ygHE7HptnjQA"
      },
      "outputs": [],
      "source": [
        "def make_dir(dir_name: str):\n",
        "    \"\"\"\n",
        "    creates directory \"dir_name\" if it doesn't exists\n",
        "    \"\"\"\n",
        "    if not os.path.exists(dir_name):\n",
        "        os.makedirs(dir_name)\n",
        "\n",
        "def custom_plot_training_stats(\n",
        "        acc_hist,\n",
        "        loss_hist,\n",
        "        phase_list,\n",
        "        title: str,\n",
        "        dir: str,\n",
        "        name: str = 'acc_loss'):\n",
        "    fig, (ax1, ax2) = plt.subplots(nrows = 1, ncols = 2, figsize=[14, 6], dpi=100)\n",
        "\n",
        "    for phase in phase_list:\n",
        "        lowest_loss_x = np.argmin(np.array(loss_hist[phase]))\n",
        "        lowest_loss_y = loss_hist[phase][lowest_loss_x]\n",
        "\n",
        "        ax1.annotate(\"{:.4f}\".format(lowest_loss_y), [lowest_loss_x, lowest_loss_y])\n",
        "        ax1.plot(loss_hist[phase], '-x', label=f'{phase} loss', markevery = [lowest_loss_x])\n",
        "\n",
        "        ax1.set_xlabel(xlabel='epochs')\n",
        "        ax1.set_ylabel(ylabel='loss')\n",
        "\n",
        "        ax1.grid(color = 'green', linestyle = '--', linewidth = 0.5, alpha=0.75)\n",
        "        ax1.legend()\n",
        "        ax1.label_outer()\n",
        "\n",
        "    # acc:\n",
        "    for phase in phase_list:\n",
        "        highest_acc_x = np.argmax(np.array(acc_hist[phase]))\n",
        "        highest_acc_y = acc_hist[phase][highest_acc_x]\n",
        "\n",
        "        ax2.annotate(\"{:.4f}\".format(highest_acc_y), [highest_acc_x, highest_acc_y])\n",
        "        ax2.plot(acc_hist[phase], '-x', label=f'{phase} acc', markevery = [highest_acc_x])\n",
        "\n",
        "        ax2.set_xlabel(xlabel='epochs')\n",
        "        ax2.set_ylabel(ylabel='acc')\n",
        "\n",
        "        ax2.grid(color = 'green', linestyle = '--', linewidth = 0.5, alpha=0.75)\n",
        "        ax2.legend()\n",
        "        #ax2.label_outer()\n",
        "\n",
        "    fig.suptitle(f'{title}')\n",
        "\n",
        "    make_dir(dir)\n",
        "    plt.savefig(f'{dir}/{name}.jpg')\n",
        "    plt.clf()\n",
        "\n",
        "def plot_conf(labels, preds, title, dir_, name):\n",
        "    \"\"\"\n",
        "    labels: an [N, ] array containing true labels for N samples\n",
        "    preds: an [N, ] array containing predications for N samples\n",
        "\n",
        "    saves confusion matrix plot of the given prediction and true labels in 'dir_/name.jpg'\n",
        "    \"\"\"\n",
        "\n",
        "    conf = confusion_matrix(labels, preds)\n",
        "\n",
        "    plt.clf()\n",
        "    cm = conf.astype('float') / conf.sum(axis=1)[:, np.newaxis]\n",
        "    cmap = sns.light_palette(\"navy\", as_cmap=True)\n",
        "    plt.figure(figsize=(20, 20))\n",
        "    sns.heatmap(cm, annot=False, cmap=cmap, fmt=\".2f\", cbar=False)\n",
        "    plt.title(f'{title}')\n",
        "    plt.xlabel('Predicted Label')\n",
        "    plt.ylabel('True Label')\n",
        "    make_dir(dir_)\n",
        "    plt.savefig(f'{dir_}/{name}')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cvgSb4Y8TDd5"
      },
      "source": [
        "## Data Analysis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Pf-5RW_2TDd5",
        "outputId": "44f89ed7-4d21-4781-e539-bc19cd858a08"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(4617, 1538, 100, 518)"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ],
      "source": [
        "len(A_train_dl.dataset), len(A_test_dl.dataset), len(B_train_dl.dataset), len(B_test_dl.dataset),"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-KIrJjCgTDd6",
        "outputId": "c2fbaabc-5fe3-4e2c-b526-443b95b8e3ae"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(19, 1, 1, 3)"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ],
      "source": [
        "len(A_train_dl), len(B_train_dl), len(B_train_dl), len(B_test_dl)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-8htOAGm_sKJ",
        "outputId": "40a32ff6-e68d-4858-f38a-6537b71e6986"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([3, 64, 64])"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ],
      "source": [
        "A_train_dl.dataset[0][0].shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gfSjAZ5ITDd6",
        "outputId": "aa6716a0-f869-4d9a-e336-6cbac7d8b958"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "data index:  0\n",
            "input:  torch.Size([3, 64, 64]) , type of input:  <class 'torch.Tensor'>\n",
            "target:  46 , type of target:  <class 'int'>\n",
            "data index:  1\n",
            "input:  torch.Size([3, 64, 64]) , type of input:  <class 'torch.Tensor'>\n",
            "target:  54 , type of target:  <class 'int'>\n",
            "data index:  2\n",
            "input:  torch.Size([3, 64, 64]) , type of input:  <class 'torch.Tensor'>\n",
            "target:  17 , type of target:  <class 'int'>\n"
          ]
        }
      ],
      "source": [
        "for data_indx, (input, target) in enumerate(A_train_dl.dataset):\n",
        "    if data_indx < 3:\n",
        "        print('data index: ', data_indx)\n",
        "        print('input: ', input.shape, ', type of input: ', type(input))\n",
        "        print('target: ', target, ', type of target: ', type(target))\n",
        "    else:\n",
        "        break"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8X58_xpiTDd8"
      },
      "source": [
        "## Making network"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "7mOnovmoTDd8"
      },
      "outputs": [],
      "source": [
        "class CNN(nn.Module):\n",
        "    def __init__(self, num_classes=80):\n",
        "        super(CNN, self).__init__()\n",
        "\n",
        "        self.flatten = nn.Flatten()\n",
        "        # conv1\n",
        "        self.conv1 = nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1)\n",
        "        self.bn1 = nn.BatchNorm2d(64)\n",
        "        self.relu1 = nn.ReLU()\n",
        "\n",
        "        # conv2\n",
        "        self.conv2_1 = nn.Conv2d(64, 64, kernel_size=3, stride=1, padding=1)\n",
        "        self.bn2_1 = nn.BatchNorm2d(64)\n",
        "        self.relu2_1 = nn.ReLU()\n",
        "\n",
        "        self.conv2_2 = nn.Conv2d(64, 64, kernel_size=3, stride=1, padding=1)\n",
        "        self.bn2_2 = nn.BatchNorm2d(64)\n",
        "        self.relu2_2 = nn.ReLU()\n",
        "\n",
        "        self.conv2_3 = nn.Conv2d(64, 64, kernel_size=3, stride=1, padding=1)\n",
        "        self.bn2_3 = nn.BatchNorm2d(64)\n",
        "        self.relu2_3 = nn.ReLU()\n",
        "\n",
        "        self.conv2_4 = nn.Conv2d(64, 64, kernel_size=3, stride=1, padding=1)\n",
        "        self.bn2_4 = nn.BatchNorm2d(64)\n",
        "        self.relu2_4 = nn.ReLU()\n",
        "\n",
        "        # pool1\n",
        "        self.pool1 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "\n",
        "        # conv3\n",
        "        self.conv3_1 = nn.Conv2d(64, 96, kernel_size=3, stride=1, padding=1)\n",
        "        self.bn3_1 = nn.BatchNorm2d(96)\n",
        "        self.relu3_1 = nn.ReLU()\n",
        "\n",
        "        self.conv3_2 = nn.Conv2d(96, 96, kernel_size=3, stride=1, padding=1)\n",
        "        self.bn3_2 = nn.BatchNorm2d(96)\n",
        "        self.relu3_2 = nn.ReLU()\n",
        "\n",
        "        self.conv3_3 = nn.Conv2d(96, 96, kernel_size=3, stride=1, padding=1)\n",
        "        self.bn3_3 = nn.BatchNorm2d(96)\n",
        "        self.relu3_3 = nn.ReLU()\n",
        "\n",
        "        self.conv3_4 = nn.Conv2d(96, 96, kernel_size=3, stride=1, padding=1)\n",
        "        self.bn3_4 = nn.BatchNorm2d(96)\n",
        "        self.relu3_4 = nn.ReLU()\n",
        "\n",
        "        # pool2\n",
        "        self.pool2 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "\n",
        "        # conv4\n",
        "        self.conv4_1 = nn.Conv2d(96, 128, kernel_size=3, stride=1, padding=1)\n",
        "        self.bn4_1 = nn.BatchNorm2d(128)\n",
        "        self.relu4_1 = nn.ReLU()\n",
        "\n",
        "        self.conv4_2 = nn.Conv2d(128, 128, kernel_size=3, stride=1, padding=1)\n",
        "        self.bn4_2 = nn.BatchNorm2d(128)\n",
        "        self.relu4_2 = nn.ReLU()\n",
        "\n",
        "        self.conv4_3 = nn.Conv2d(128, 128, kernel_size=3, stride=1, padding=1)\n",
        "        self.bn4_3 = nn.BatchNorm2d(128)\n",
        "        self.relu4_3 = nn.ReLU()\n",
        "\n",
        "        self.conv4_4 = nn.Conv2d(128, 128, kernel_size=3, stride=1, padding=1)\n",
        "        self.bn4_4 = nn.BatchNorm2d(128)\n",
        "        self.relu4_4 = nn.ReLU()\n",
        "\n",
        "        # pool3\n",
        "        self.pool3 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "\n",
        "        # conv5\n",
        "        self.conv5_1 = nn.Conv2d(128, 256, kernel_size=3, stride=1, padding=1)\n",
        "        self.bn5_1 = nn.BatchNorm2d(256)\n",
        "        self.relu5_1 = nn.ReLU()\n",
        "\n",
        "        self.conv5_2 = nn.Conv2d(256, 256, kernel_size=3, stride=1, padding=1)\n",
        "        self.bn5_2 = nn.BatchNorm2d(256)\n",
        "        self.relu5_2 = nn.ReLU()\n",
        "\n",
        "        self.conv5_3 = nn.Conv2d(256, 256, kernel_size=3, stride=1, padding=1)\n",
        "        self.bn5_3 = nn.BatchNorm2d(256)\n",
        "        self.relu5_3 = nn.ReLU()\n",
        "\n",
        "        self.conv5_4 = nn.Conv2d(256, 256, kernel_size=3, stride=1, padding=1)\n",
        "        self.bn5_4 = nn.BatchNorm2d(256)\n",
        "        self.relu5_4 = nn.ReLU()\n",
        "\n",
        "        # pool4\n",
        "        self.pool4 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "\n",
        "        # foolly connected\n",
        "        self.flatten = nn.Flatten()\n",
        "        self.fc = nn.Linear(256*4*4, num_classes)\n",
        "\n",
        "    def forward(self, inputs, debug=False):\n",
        "        # conv 1\n",
        "        conv1 = self.conv1(inputs)\n",
        "        bn1 = self.bn1(conv1)\n",
        "        relu1 = self.relu1(bn1)\n",
        "\n",
        "        # conv 2\n",
        "        conv2_1 = self.conv2_1(relu1)\n",
        "        bn2_1 = self.bn2_1(conv2_1)\n",
        "        relu2_1 = self.relu2_1(bn2_1)\n",
        "\n",
        "        conv2_2 = self.conv2_2(relu2_1)\n",
        "        bn2_2 = self.bn2_2(conv2_2)\n",
        "        relu2_2 = self.relu2_2(bn2_2)\n",
        "\n",
        "        conv2_3 = self.conv2_3(relu2_2)\n",
        "        bn2_3 = self.bn2_3(conv2_3)\n",
        "        relu2_3 = self.relu2_3(bn2_3)\n",
        "\n",
        "        conv2_4 = self.conv2_4(relu2_3)\n",
        "        bn2_4 = self.bn2_4(conv2_4)\n",
        "        relu2_4 = self.relu2_4(bn2_4)\n",
        "\n",
        "        # pool 1\n",
        "        pool1 = self.pool1(relu2_4)\n",
        "\n",
        "        # conv 3\n",
        "        conv3_1 = self.conv3_1(pool1)\n",
        "        bn3_1 = self.bn3_1(conv3_1)\n",
        "        relu3_1 = self.relu3_1(bn3_1)\n",
        "\n",
        "        conv3_2 = self.conv3_2(relu3_1)\n",
        "        bn3_2 = self.bn3_2(conv3_2)\n",
        "        relu3_2 = self.relu3_2(bn3_2)\n",
        "\n",
        "        conv3_3 = self.conv3_3(relu3_2)\n",
        "        bn3_3 = self.bn3_3(conv3_3)\n",
        "        relu3_3 = self.relu3_3(bn3_3)\n",
        "\n",
        "        conv3_4 = self.conv3_4(relu3_3)\n",
        "        bn3_4 = self.bn3_4(conv3_4)\n",
        "        relu3_4 = self.relu3_4(bn3_4)\n",
        "\n",
        "        # pool 2\n",
        "        pool2 = self.pool2(relu3_4)\n",
        "\n",
        "        # conv 4\n",
        "        conv4_1 = self.conv4_1(pool2)\n",
        "        bn4_1 = self.bn4_1(conv4_1)\n",
        "        relu4_1 = self.relu4_1(bn4_1)\n",
        "\n",
        "        conv4_2 = self.conv4_2(relu4_1)\n",
        "        bn4_2 = self.bn4_2(conv4_2)\n",
        "        relu4_2 = self.relu4_2(bn4_2)\n",
        "\n",
        "        conv4_3 = self.conv4_3(relu4_2)\n",
        "        bn4_3 = self.bn4_3(conv4_3)\n",
        "        relu4_3 = self.relu4_3(bn4_3)\n",
        "\n",
        "        conv4_4 = self.conv4_4(relu4_3)\n",
        "        bn4_4 = self.bn4_4(conv4_4)\n",
        "        relu4_4 = self.relu4_4(bn4_4)\n",
        "\n",
        "        # pool 3\n",
        "        pool3 = self.pool3(relu4_4)\n",
        "\n",
        "        # conv 5\n",
        "        conv5_1 = self.conv5_1(pool3)\n",
        "        bn5_1 = self.bn5_1(conv5_1)\n",
        "        relu5_1 = self.relu5_1(bn5_1)\n",
        "\n",
        "        conv5_2 = self.conv5_2(relu5_1)\n",
        "        bn5_2 = self.bn5_2(conv5_2)\n",
        "        relu5_2 = self.relu5_2(bn5_2)\n",
        "\n",
        "        conv5_3 = self.conv5_3(relu5_2)\n",
        "        bn5_3 = self.bn5_3(conv5_3)\n",
        "        relu5_3 = self.relu5_3(bn5_3)\n",
        "\n",
        "        conv5_4 = self.conv5_4(relu5_3)\n",
        "        bn5_4 = self.bn5_4(conv5_4)\n",
        "        relu5_4 = self.relu5_4(bn5_4)\n",
        "\n",
        "        # pool 4\n",
        "        pool4 = self.pool4(relu5_4)\n",
        "\n",
        "        # fc\n",
        "        flatten = self.flatten(pool4)\n",
        "        fc = self.fc(flatten)\n",
        "\n",
        "        return(fc)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "YeqmiNTUUfhM"
      },
      "outputs": [],
      "source": [
        "# DataLoader.datasets"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l_GDS8aMyxlJ"
      },
      "source": [
        "## Train model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "CgXu7x7bzIJr"
      },
      "outputs": [],
      "source": [
        "def train_one_epoch(model: nn.Module, optim: torch.optim.Optimizer,\n",
        "                    dataloader: DataLoader, loss_fn):\n",
        "\n",
        "    num_samples = len(dataloader.dataset)\n",
        "    num_batches = len(dataloader)\n",
        "    running_corrects = 0\n",
        "    running_loss = 0.0\n",
        "\n",
        "    model.train()\n",
        "\n",
        "    for batch_indx, (inputs, targets) in enumerate(dataloader): # Get a batch of Data\n",
        "        inputs = inputs.to(device)\n",
        "        targets = targets.to(device)\n",
        "\n",
        "        outputs = model(inputs) # Forward Pass\n",
        "        loss = loss_fn(outputs, targets) # Compute Loss\n",
        "\n",
        "        loss.backward() # Compute Gradients\n",
        "        optim.step() # Update parameters\n",
        "        optim.zero_grad() # zero the parameter's gradients\n",
        "\n",
        "        _, preds = torch.max(outputs, dim=1)\n",
        "        running_corrects += torch.sum(preds == targets).cpu()\n",
        "        running_loss += loss.item()\n",
        "\n",
        "        # print(f\"< TRAIN >--< {batch_indx} >-------------------------------\")\n",
        "        # print(f\"out:\\n{outputs}\")\n",
        "        # print(f\"tar:\\n{target_tensor}\")\n",
        "        # print(f\"running_corrects:\\n{running_corrects}\")\n",
        "        # print(f\"running_loss:\\n{running_loss}\")\n",
        "\n",
        "    epoch_acc = (running_corrects / num_samples) * 100\n",
        "    print(running_corrects, num_samples)\n",
        "    epoch_loss = (running_loss / num_batches)\n",
        "\n",
        "    return epoch_acc, epoch_loss"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C9F2sjhX2e9t"
      },
      "source": [
        "## Test model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "pPiBHmCp0jyC"
      },
      "outputs": [],
      "source": [
        "def test_model(model: nn.Module,\n",
        "               dataloader: DataLoader, loss_fn):\n",
        "\n",
        "    num_samples = len(dataloader.dataset)\n",
        "    num_batches = len(dataloader)\n",
        "    running_corrects = 0\n",
        "    running_loss = 0.0\n",
        "\n",
        "    # we call `model.eval()` to set dropout and batch normalization layers to evaluation mode before running inference.\n",
        "    model.eval()\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch_indx, (inputs, targets) in enumerate(dataloader): # Get a batch of Data\n",
        "            inputs = inputs.to(device)\n",
        "            targets = targets.to(device)\n",
        "\n",
        "            outputs = model(inputs) # Forward Pass\n",
        "            loss = loss_fn(outputs, targets) # Compute Loss\n",
        "\n",
        "            _, preds = torch.max(outputs, 1) #\n",
        "            running_corrects += torch.sum(preds == targets).cpu()\n",
        "            running_loss += loss.item()\n",
        "\n",
        "            # print(f\"< TEST >--< {batch_indx} >-------------------------------\")\n",
        "            # print(f\"out:\\n{outputs}\")\n",
        "            # print(f\"tar:\\n{target_tensor}\")\n",
        "            # print(f\"running_corrects:\\n{running_corrects}\")\n",
        "            # print(f\"running_loss:\\n{running_loss}\")\n",
        "\n",
        "    test_acc = (running_corrects / num_samples) * 100\n",
        "    test_loss = (running_loss / num_batches)\n",
        "\n",
        "    return test_acc, test_loss"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pjWz5rRn2jlp"
      },
      "source": [
        "## Evaluate model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "bAexh8NL1SOm"
      },
      "outputs": [],
      "source": [
        "def evaluate():\n",
        "    num_epochs = 30\n",
        "    learning_rate = 0.0005\n",
        "\n",
        "    full_dataloaders = {\n",
        "        'train': A_train_dl,\n",
        "        'test': A_test_dl\n",
        "    }\n",
        "\n",
        "    model = CNN()\n",
        "    model = model.to(device)\n",
        "\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
        "    cross_entropy = nn.CrossEntropyLoss()\n",
        "\n",
        "    acc_history = {'train': [], 'test': []}\n",
        "    loss_history = {'train': [], 'test': []}\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        train_acc, train_loss = train_one_epoch(model=model, optim=optimizer, dataloader=full_dataloaders['train'], loss_fn=cross_entropy)\n",
        "        test_acc, test_loss = test_model(model=model, dataloader=full_dataloaders['test'], loss_fn=cross_entropy)\n",
        "\n",
        "        acc_history['train'].append(train_acc)\n",
        "        acc_history['test'].append(test_acc)\n",
        "        loss_history['train'].append(train_loss)\n",
        "        loss_history['test'].append(test_loss)\n",
        "\n",
        "        print(f\"---------< epoch: {epoch} >---------\")\n",
        "\n",
        "    # # save model\n",
        "    model_path = './CNN_model_ph1.pth'\n",
        "    torch.save(model.state_dict(), model_path)\n",
        "\n",
        "    # plot accuracy and loss\n",
        "    custom_plot_training_stats(acc_history, loss_history, ['train', 'test'], title='demp', dir='demo_plots')\n",
        "\n",
        "    # show the details of model\n",
        "    batch_size=128\n",
        "    print(summary(model, input_size=(batch_size, 3, 64, 64)))\n",
        "\n",
        "    return (acc_history, loss_history)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "3tgjkkym8zzJ",
        "outputId": "aa9338ff-f4d1-46e4-e1db-73606cc8e3af"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor(258) 4617\n",
            "---------< epoch: 0 >---------\n",
            "tensor(594) 4617\n",
            "---------< epoch: 1 >---------\n",
            "tensor(1026) 4617\n",
            "---------< epoch: 2 >---------\n",
            "tensor(1405) 4617\n",
            "---------< epoch: 3 >---------\n",
            "tensor(1828) 4617\n",
            "---------< epoch: 4 >---------\n",
            "tensor(2211) 4617\n",
            "---------< epoch: 5 >---------\n",
            "tensor(2571) 4617\n",
            "---------< epoch: 6 >---------\n",
            "tensor(2721) 4617\n",
            "---------< epoch: 7 >---------\n",
            "tensor(2999) 4617\n",
            "---------< epoch: 8 >---------\n",
            "tensor(3319) 4617\n",
            "---------< epoch: 9 >---------\n",
            "tensor(3606) 4617\n",
            "---------< epoch: 10 >---------\n",
            "tensor(3798) 4617\n",
            "---------< epoch: 11 >---------\n",
            "tensor(3972) 4617\n",
            "---------< epoch: 12 >---------\n",
            "tensor(4113) 4617\n",
            "---------< epoch: 13 >---------\n",
            "tensor(4230) 4617\n",
            "---------< epoch: 14 >---------\n",
            "tensor(4355) 4617\n",
            "---------< epoch: 15 >---------\n",
            "tensor(4459) 4617\n",
            "---------< epoch: 16 >---------\n",
            "tensor(4517) 4617\n",
            "---------< epoch: 17 >---------\n",
            "tensor(4550) 4617\n",
            "---------< epoch: 18 >---------\n",
            "tensor(4522) 4617\n",
            "---------< epoch: 19 >---------\n",
            "tensor(4508) 4617\n",
            "---------< epoch: 20 >---------\n",
            "tensor(4572) 4617\n",
            "---------< epoch: 21 >---------\n",
            "tensor(4578) 4617\n",
            "---------< epoch: 22 >---------\n",
            "tensor(4598) 4617\n",
            "---------< epoch: 23 >---------\n",
            "tensor(4613) 4617\n",
            "---------< epoch: 24 >---------\n",
            "tensor(4617) 4617\n",
            "---------< epoch: 25 >---------\n",
            "tensor(4617) 4617\n",
            "---------< epoch: 26 >---------\n",
            "tensor(4617) 4617\n",
            "---------< epoch: 27 >---------\n",
            "tensor(4617) 4617\n",
            "---------< epoch: 28 >---------\n",
            "tensor(4617) 4617\n",
            "---------< epoch: 29 >---------\n",
            "==========================================================================================\n",
            "Layer (type:depth-idx)                   Output Shape              Param #\n",
            "==========================================================================================\n",
            "CNN                                      [128, 80]                 --\n",
            "├─Conv2d: 1-1                            [128, 64, 64, 64]         1,792\n",
            "├─BatchNorm2d: 1-2                       [128, 64, 64, 64]         128\n",
            "├─ReLU: 1-3                              [128, 64, 64, 64]         --\n",
            "├─Conv2d: 1-4                            [128, 64, 64, 64]         36,928\n",
            "├─BatchNorm2d: 1-5                       [128, 64, 64, 64]         128\n",
            "├─ReLU: 1-6                              [128, 64, 64, 64]         --\n",
            "├─Conv2d: 1-7                            [128, 64, 64, 64]         36,928\n",
            "├─BatchNorm2d: 1-8                       [128, 64, 64, 64]         128\n",
            "├─ReLU: 1-9                              [128, 64, 64, 64]         --\n",
            "├─Conv2d: 1-10                           [128, 64, 64, 64]         36,928\n",
            "├─BatchNorm2d: 1-11                      [128, 64, 64, 64]         128\n",
            "├─ReLU: 1-12                             [128, 64, 64, 64]         --\n",
            "├─Conv2d: 1-13                           [128, 64, 64, 64]         36,928\n",
            "├─BatchNorm2d: 1-14                      [128, 64, 64, 64]         128\n",
            "├─ReLU: 1-15                             [128, 64, 64, 64]         --\n",
            "├─MaxPool2d: 1-16                        [128, 64, 32, 32]         --\n",
            "├─Conv2d: 1-17                           [128, 96, 32, 32]         55,392\n",
            "├─BatchNorm2d: 1-18                      [128, 96, 32, 32]         192\n",
            "├─ReLU: 1-19                             [128, 96, 32, 32]         --\n",
            "├─Conv2d: 1-20                           [128, 96, 32, 32]         83,040\n",
            "├─BatchNorm2d: 1-21                      [128, 96, 32, 32]         192\n",
            "├─ReLU: 1-22                             [128, 96, 32, 32]         --\n",
            "├─Conv2d: 1-23                           [128, 96, 32, 32]         83,040\n",
            "├─BatchNorm2d: 1-24                      [128, 96, 32, 32]         192\n",
            "├─ReLU: 1-25                             [128, 96, 32, 32]         --\n",
            "├─Conv2d: 1-26                           [128, 96, 32, 32]         83,040\n",
            "├─BatchNorm2d: 1-27                      [128, 96, 32, 32]         192\n",
            "├─ReLU: 1-28                             [128, 96, 32, 32]         --\n",
            "├─MaxPool2d: 1-29                        [128, 96, 16, 16]         --\n",
            "├─Conv2d: 1-30                           [128, 128, 16, 16]        110,720\n",
            "├─BatchNorm2d: 1-31                      [128, 128, 16, 16]        256\n",
            "├─ReLU: 1-32                             [128, 128, 16, 16]        --\n",
            "├─Conv2d: 1-33                           [128, 128, 16, 16]        147,584\n",
            "├─BatchNorm2d: 1-34                      [128, 128, 16, 16]        256\n",
            "├─ReLU: 1-35                             [128, 128, 16, 16]        --\n",
            "├─Conv2d: 1-36                           [128, 128, 16, 16]        147,584\n",
            "├─BatchNorm2d: 1-37                      [128, 128, 16, 16]        256\n",
            "├─ReLU: 1-38                             [128, 128, 16, 16]        --\n",
            "├─Conv2d: 1-39                           [128, 128, 16, 16]        147,584\n",
            "├─BatchNorm2d: 1-40                      [128, 128, 16, 16]        256\n",
            "├─ReLU: 1-41                             [128, 128, 16, 16]        --\n",
            "├─MaxPool2d: 1-42                        [128, 128, 8, 8]          --\n",
            "├─Conv2d: 1-43                           [128, 256, 8, 8]          295,168\n",
            "├─BatchNorm2d: 1-44                      [128, 256, 8, 8]          512\n",
            "├─ReLU: 1-45                             [128, 256, 8, 8]          --\n",
            "├─Conv2d: 1-46                           [128, 256, 8, 8]          590,080\n",
            "├─BatchNorm2d: 1-47                      [128, 256, 8, 8]          512\n",
            "├─ReLU: 1-48                             [128, 256, 8, 8]          --\n",
            "├─Conv2d: 1-49                           [128, 256, 8, 8]          590,080\n",
            "├─BatchNorm2d: 1-50                      [128, 256, 8, 8]          512\n",
            "├─ReLU: 1-51                             [128, 256, 8, 8]          --\n",
            "├─Conv2d: 1-52                           [128, 256, 8, 8]          590,080\n",
            "├─BatchNorm2d: 1-53                      [128, 256, 8, 8]          512\n",
            "├─ReLU: 1-54                             [128, 256, 8, 8]          --\n",
            "├─MaxPool2d: 1-55                        [128, 256, 4, 4]          --\n",
            "├─Flatten: 1-56                          [128, 4096]               --\n",
            "├─Linear: 1-57                           [128, 80]                 327,760\n",
            "==========================================================================================\n",
            "Total params: 3,405,136\n",
            "Trainable params: 3,405,136\n",
            "Non-trainable params: 0\n",
            "Total mult-adds (G): 153.39\n",
            "==========================================================================================\n",
            "Input size (MB): 6.29\n",
            "Forward/backward pass size (MB): 3892.40\n",
            "Params size (MB): 13.62\n",
            "Estimated Total Size (MB): 3912.31\n",
            "==========================================================================================\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1400x600 with 0 Axes>"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "acc_history, loss_history = evaluate()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SwMi4-EZGXOv"
      },
      "source": [
        "# phase 2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "azb98FW5JEi7"
      },
      "source": [
        "## loading phase1 model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "lZaeoFyHaF_j",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "05d56b24-f477-47e9-838a-db1a08dedf82"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ],
      "source": [
        "model2 = CNN()\n",
        "model2 = model2.to(device)\n",
        "\n",
        "model_path = './CNN_model_ph1.pth'\n",
        "model2.load_state_dict(torch.load(model_path))"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "6VoGLQNV0AOX",
        "cvgSb4Y8TDd5",
        "8X58_xpiTDd8",
        "C9F2sjhX2e9t"
      ],
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}