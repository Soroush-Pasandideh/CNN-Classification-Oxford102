{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {
        "id": "DRxbohbATDdy"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision\n",
        "# import utils\n",
        "from typing import List\n",
        "from torchvision import transforms, models, datasets\n",
        "import torch\n",
        "from torch.utils.data import DataLoader, Subset, random_split, ConcatDataset\n",
        "import numpy as np\n",
        "import random\n",
        "from os.path import exists\n",
        "import os\n",
        "from typing import List, Dict, Tuple\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix\n",
        "from sklearn.metrics import ConfusionMatrixDisplay\n",
        "import seaborn as sns\n",
        "from sklearn.utils.multiclass import unique_labels\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vcJYNoSIqHWw",
        "outputId": "a67c9d3c-3b0e-41b0-fd3e-c40690e2a25a"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "device(type='cuda', index=0)"
            ]
          },
          "metadata": {},
          "execution_count": 52
        }
      ],
      "source": [
        "use_gpu = True\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() and use_gpu else \"cpu\")\n",
        "# device = torch.device(\"cpu\")\n",
        "device"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0GIL20ZmUfhD",
        "outputId": "9568dc1d-b0bf-4697-eb12-7efa8e29354d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CUDA available:  True\n",
            "Code running on GPU:  True\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "\n",
        "# Check if CUDA (GPU support) is available\n",
        "print(\"CUDA available: \", torch.cuda.is_available())\n",
        "\n",
        "# Check if code is running on GPU\n",
        "print(\"Code running on GPU: \", torch.cuda.is_initialized())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B03Ai5UkTDd0"
      },
      "source": [
        "## Data Loader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {
        "id": "F07yw0wNTDd2"
      },
      "outputs": [],
      "source": [
        "def get_oxford_splits(\n",
        "    batch_size: int,\n",
        "    data_loader_seed: int = 111,\n",
        "    pin_memory: bool = True,\n",
        "    num_workers: int = 2,\n",
        "    ):\n",
        "    K = 5\n",
        "    num_support = 80\n",
        "    num_query = 20\n",
        "\n",
        "    def seed_worker(worker_id):\n",
        "        # worker_seed = torch.initial_seed() % 2 ** 32\n",
        "        np.random.seed(data_loader_seed)\n",
        "        random.seed(data_loader_seed)\n",
        "    g = torch.Generator()\n",
        "    g.manual_seed(data_loader_seed)\n",
        "\n",
        "    support_classes = list(np.arange(num_support))\n",
        "    query_classes = list(np.arange(num_query) + num_support)\n",
        "\n",
        "    img_dim = 64\n",
        "\n",
        "    train_transforms = transforms.Compose([\n",
        "        transforms.Resize((img_dim, img_dim)),\n",
        "        transforms.RandomHorizontalFlip(),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize([0.485, 0.456, 0.406],[0.229, 0.224, 0.225])])\n",
        "    test_transforms = transforms.Compose([\n",
        "        transforms.Resize((img_dim, img_dim)),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize([0.485, 0.456, 0.406],[0.229, 0.224, 0.225])])\n",
        "    validation_transforms = transforms.Compose([\n",
        "        transforms.Resize((img_dim, img_dim)),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize([0.485, 0.456, 0.406],[0.229, 0.224, 0.225])])\n",
        "\n",
        "    data_path = f'/content/data'\n",
        "\n",
        "    train_ds_full = datasets.Flowers102(root=data_path, split=\"train\", download=True, transform=train_transforms)\n",
        "    val_ds_full = datasets.Flowers102(root=data_path, split=\"val\", download=True, transform=validation_transforms)\n",
        "    test_ds_full = datasets.Flowers102(root=data_path, split=\"test\", download=True, transform=test_transforms)\n",
        "\n",
        "    train_indxs_support = torch.where(torch.isin(torch.tensor(train_ds_full._labels), torch.asarray(support_classes)))[0]\n",
        "    val_indxs_support = torch.where(torch.isin(torch.tensor(val_ds_full._labels), torch.asarray(support_classes)))[0]\n",
        "    test_indxs_support = torch.where(torch.isin(torch.tensor(test_ds_full._labels), torch.asarray(support_classes)))[0]\n",
        "\n",
        "    train_ds_subset_support = torch.utils.data.Subset(train_ds_full, train_indxs_support)\n",
        "    val_ds_subset_support = torch.utils.data.Subset(val_ds_full, val_indxs_support)\n",
        "    test_ds_subset_support = torch.utils.data.Subset(test_ds_full, test_indxs_support)\n",
        "\n",
        "    merged_dataset = ConcatDataset([train_ds_subset_support, val_ds_subset_support, test_ds_subset_support])\n",
        "    ### A, B\n",
        "    train_ds_support, test_ds_support = torch.utils.data.random_split(merged_dataset, [0.75, 0.25], generator=torch.Generator().manual_seed(42))\n",
        "    ###\n",
        "\n",
        "    train_indxs_query = torch.where(torch.isin(torch.tensor(train_ds_full._labels), torch.asarray(query_classes)))[0]\n",
        "    N = 10\n",
        "    starting_indices = np.arange(0, len(train_indxs_query), N)\n",
        "    train_indxs_query = np.hstack([train_indxs_query[i:i+K] for i in starting_indices if i + K <= len(train_indxs_query)])\n",
        "    ### C\n",
        "    train_ds_query = torch.utils.data.Subset(train_ds_full, train_indxs_query)\n",
        "    ###\n",
        "\n",
        "    val_indxs_query = torch.where(torch.isin(torch.tensor(val_ds_full._labels), torch.asarray(query_classes)))[0]\n",
        "    test_indxs_query = torch.where(torch.isin(torch.tensor(test_ds_full._labels), torch.asarray(query_classes)))[0]\n",
        "    val_ds_subset_query = torch.utils.data.Subset(val_ds_full, val_indxs_query)\n",
        "    test_ds_subset_query = torch.utils.data.Subset(test_ds_full, test_indxs_query)\n",
        "\n",
        "    test_ds_query_full = ConcatDataset([val_ds_subset_query, test_ds_subset_query])\n",
        "    ### D\n",
        "    _, test_ds_query = torch.utils.data.random_split(test_ds_query_full, [0.7, 0.3], generator=torch.Generator().manual_seed(42))\n",
        "    ###\n",
        "\n",
        "    ### E\n",
        "    test_all = ConcatDataset([test_ds_query, test_ds_support])\n",
        "\n",
        "\n",
        "    A_train_dl = DataLoader(\n",
        "        train_ds_support,\n",
        "        batch_size = batch_size,\n",
        "        shuffle=True,\n",
        "        worker_init_fn=seed_worker,\n",
        "        generator=g,\n",
        "        drop_last=False,\n",
        "        pin_memory=pin_memory,\n",
        "        num_workers=num_workers\n",
        "    )\n",
        "    A_test_dl = DataLoader(\n",
        "        test_ds_support,\n",
        "        batch_size = batch_size,\n",
        "        shuffle=True,\n",
        "        worker_init_fn=seed_worker,\n",
        "        generator=g,\n",
        "        drop_last=False,\n",
        "        pin_memory=pin_memory,\n",
        "        num_workers=num_workers\n",
        "    )\n",
        "\n",
        "    B_train_dl = DataLoader(\n",
        "        train_ds_query,\n",
        "        batch_size = batch_size,\n",
        "        shuffle=True,\n",
        "        worker_init_fn=seed_worker,\n",
        "        generator=g,\n",
        "        drop_last=False,\n",
        "        pin_memory=pin_memory,\n",
        "        num_workers=num_workers\n",
        "    )\n",
        "    B_test_dl = DataLoader(\n",
        "        test_ds_query,\n",
        "        batch_size = batch_size,\n",
        "        shuffle=True,\n",
        "        worker_init_fn=seed_worker,\n",
        "        generator=g,\n",
        "        drop_last=False,\n",
        "        pin_memory=pin_memory,\n",
        "        num_workers=num_workers\n",
        "    )\n",
        "    test_all = DataLoader(\n",
        "        test_all,\n",
        "        batch_size = batch_size,\n",
        "        shuffle=True,\n",
        "        worker_init_fn=seed_worker,\n",
        "        generator=g,\n",
        "        drop_last=False,\n",
        "        pin_memory=pin_memory,\n",
        "        num_workers=num_workers\n",
        "    )\n",
        "\n",
        "    return A_train_dl, A_test_dl, B_train_dl, B_test_dl, test_all"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6VoGLQNV0AOX"
      },
      "source": [
        "## Split data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {
        "id": "nqwdU4e3TDd4"
      },
      "outputs": [],
      "source": [
        "A_train_dl, A_test_dl, B_train_dl, B_test_dl, test_all = get_oxford_splits(\n",
        "    batch_size=128,\n",
        "    data_loader_seed=111,\n",
        "    pin_memory=False,\n",
        "    num_workers=1\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ryLMC8ZYnkou"
      },
      "source": [
        "## plot output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {
        "id": "ygHE7HptnjQA"
      },
      "outputs": [],
      "source": [
        "def make_dir(dir_name: str):\n",
        "    \"\"\"\n",
        "    creates directory \"dir_name\" if it doesn't exists\n",
        "    \"\"\"\n",
        "    if not os.path.exists(dir_name):\n",
        "        os.makedirs(dir_name)\n",
        "\n",
        "def custom_plot_training_stats(\n",
        "        acc_hist,\n",
        "        loss_hist,\n",
        "        phase_list,\n",
        "        title: str,\n",
        "        dir: str,\n",
        "        name: str = 'acc_loss'):\n",
        "    fig, (ax1, ax2) = plt.subplots(nrows = 1, ncols = 2, figsize=[14, 6], dpi=100)\n",
        "\n",
        "    for phase in phase_list:\n",
        "        lowest_loss_x = np.argmin(np.array(loss_hist[phase]))\n",
        "        lowest_loss_y = loss_hist[phase][lowest_loss_x]\n",
        "\n",
        "        ax1.annotate(\"{:.4f}\".format(lowest_loss_y), [lowest_loss_x, lowest_loss_y])\n",
        "        ax1.plot(loss_hist[phase], '-x', label=f'{phase} loss', markevery = [lowest_loss_x])\n",
        "\n",
        "        ax1.set_xlabel(xlabel='epochs')\n",
        "        ax1.set_ylabel(ylabel='loss')\n",
        "\n",
        "        ax1.grid(color = 'green', linestyle = '--', linewidth = 0.5, alpha=0.75)\n",
        "        ax1.legend()\n",
        "        ax1.label_outer()\n",
        "\n",
        "    # acc:\n",
        "    for phase in phase_list:\n",
        "        highest_acc_x = np.argmax(np.array(acc_hist[phase]))\n",
        "        highest_acc_y = acc_hist[phase][highest_acc_x]\n",
        "\n",
        "        ax2.annotate(\"{:.4f}\".format(highest_acc_y), [highest_acc_x, highest_acc_y])\n",
        "        ax2.plot(acc_hist[phase], '-x', label=f'{phase} acc', markevery = [highest_acc_x])\n",
        "\n",
        "        ax2.set_xlabel(xlabel='epochs')\n",
        "        ax2.set_ylabel(ylabel='acc')\n",
        "\n",
        "        ax2.grid(color = 'green', linestyle = '--', linewidth = 0.5, alpha=0.75)\n",
        "        ax2.legend()\n",
        "        #ax2.label_outer()\n",
        "\n",
        "    fig.suptitle(f'{title}')\n",
        "\n",
        "    make_dir(dir)\n",
        "    plt.savefig(f'{dir}/{name}.jpg')\n",
        "    plt.clf()\n",
        "\n",
        "def plot_conf(labels, preds, title, dir_, name):\n",
        "    \"\"\"\n",
        "    labels: an [N, ] array containing true labels for N samples\n",
        "    preds: an [N, ] array containing predications for N samples\n",
        "\n",
        "    saves confusion matrix plot of the given prediction and true labels in 'dir_/name.jpg'\n",
        "    \"\"\"\n",
        "\n",
        "    conf = confusion_matrix(labels, preds)\n",
        "\n",
        "    plt.clf()\n",
        "    cm = conf.astype('float') / conf.sum(axis=1)[:, np.newaxis]\n",
        "    cmap = sns.light_palette(\"navy\", as_cmap=True)\n",
        "    plt.figure(figsize=(20, 20))\n",
        "    sns.heatmap(cm, annot=False, cmap=cmap, fmt=\".2f\", cbar=False)\n",
        "    plt.title(f'{title}')\n",
        "    plt.xlabel('Predicted Label')\n",
        "    plt.ylabel('True Label')\n",
        "    make_dir(dir_)\n",
        "    plt.savefig(f'{dir_}/{name}')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cvgSb4Y8TDd5"
      },
      "source": [
        "## Data Analysis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Pf-5RW_2TDd5",
        "outputId": "4a87f94f-7de6-485e-9b43-7aaaf84c8f00"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(4617, 1538, 100, 518)"
            ]
          },
          "metadata": {},
          "execution_count": 57
        }
      ],
      "source": [
        "len(A_train_dl.dataset), len(A_test_dl.dataset), len(B_train_dl.dataset), len(B_test_dl.dataset),"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-KIrJjCgTDd6",
        "outputId": "5844c34b-ebae-4267-9b19-82b65f677bdf"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(37, 1, 1, 5)"
            ]
          },
          "metadata": {},
          "execution_count": 58
        }
      ],
      "source": [
        "len(A_train_dl), len(B_train_dl), len(B_train_dl), len(B_test_dl)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-8htOAGm_sKJ",
        "outputId": "1b30324d-b900-4693-aa03-a0eede1a97d0"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([3, 64, 64])"
            ]
          },
          "metadata": {},
          "execution_count": 59
        }
      ],
      "source": [
        "A_train_dl.dataset[0][0].shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gfSjAZ5ITDd6",
        "outputId": "d213e0da-f3f2-41da-c944-d2ab43a3498e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "data index:  0\n",
            "input:  torch.Size([3, 64, 64]) , type of input:  <class 'torch.Tensor'>\n",
            "target:  46 , type of target:  <class 'int'>\n",
            "data index:  1\n",
            "input:  torch.Size([3, 64, 64]) , type of input:  <class 'torch.Tensor'>\n",
            "target:  54 , type of target:  <class 'int'>\n",
            "data index:  2\n",
            "input:  torch.Size([3, 64, 64]) , type of input:  <class 'torch.Tensor'>\n",
            "target:  17 , type of target:  <class 'int'>\n"
          ]
        }
      ],
      "source": [
        "for data_indx, (input, target) in enumerate(A_train_dl.dataset):\n",
        "    if data_indx < 3:\n",
        "        print('data index: ', data_indx)\n",
        "        print('input: ', input.shape, ', type of input: ', type(input))\n",
        "        print('target: ', target, ', type of target: ', type(target))\n",
        "    else:\n",
        "        break"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8X58_xpiTDd8"
      },
      "source": [
        "## Making network"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "metadata": {
        "id": "7mOnovmoTDd8"
      },
      "outputs": [],
      "source": [
        "class CNN(nn.Module):\n",
        "    def __init__(self, num_classes=80):\n",
        "        super(CNN, self).__init__()\n",
        "\n",
        "        self.flatten = nn.Flatten()\n",
        "\n",
        "        self.conv1 = nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1)\n",
        "        self.bn1 = nn.BatchNorm2d(64)\n",
        "        self.relu1 = nn.ReLU()\n",
        "\n",
        "        self.conv2 = nn.Conv2d(64, 64, kernel_size=3, stride=1, padding=1)\n",
        "        self.bn2 = nn.BatchNorm2d(64)\n",
        "        self.relu2 = nn.ReLU()\n",
        "\n",
        "        self.pool1 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "\n",
        "        self.conv3 = nn.Conv2d(64, 96, kernel_size=3, stride=1, padding=1)\n",
        "        self.conv3_2 = nn.Conv2d(96, 96, kernel_size=3, stride=1, padding=1)\n",
        "        self.bn3 = nn.BatchNorm2d(96)\n",
        "        self.relu3 = nn.ReLU()\n",
        "\n",
        "        self.pool2 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "\n",
        "        self.conv4 = nn.Conv2d(96, 128, kernel_size=3, stride=1, padding=1)\n",
        "        self.conv4_2 = nn.Conv2d(128, 128, kernel_size=3, stride=1, padding=1)\n",
        "        self.bn4 = nn.BatchNorm2d(128)\n",
        "        self.relu4 = nn.ReLU()\n",
        "\n",
        "        self.pool3 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "\n",
        "        self.conv5 = nn.Conv2d(128, 256, kernel_size=3, stride=1, padding=1)\n",
        "        self.conv5_2 = nn.Conv2d(256, 256, kernel_size=3, stride=1, padding=1)\n",
        "        self.bn5 = nn.BatchNorm2d(256)\n",
        "        self.relu5 = nn.ReLU()\n",
        "\n",
        "        self.pool4 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "\n",
        "        self.flatten = nn.Flatten()\n",
        "\n",
        "        self.fc = nn.Linear(256*4*4, num_classes)\n",
        "\n",
        "    def forward(self, inputs, debug=False):\n",
        "        # conv 1\n",
        "        conv1 = self.conv1(inputs)\n",
        "        # conv1 = self.conv1(inputs.unsqueeze(0))\n",
        "        bn1 = self.bn1(conv1)\n",
        "        relu1 = self.relu1(bn1)\n",
        "\n",
        "        # conv 2\n",
        "        conv2_1 = self.conv2(relu1)\n",
        "        bn2_1 = self.bn2(conv2_1)\n",
        "        relu2_1 = self.relu2(bn2_1)\n",
        "\n",
        "        conv2_2 = self.conv2(relu2_1)\n",
        "        bn2_2 = self.bn2(conv2_2)\n",
        "        relu2_2 = self.relu2(bn2_2)\n",
        "\n",
        "        conv2_3 = self.conv2(relu2_2)\n",
        "        bn2_3 = self.bn2(conv2_3)\n",
        "        relu2_3 = self.relu2(bn2_3)\n",
        "\n",
        "        conv2_4 = self.conv2(relu2_3)\n",
        "        bn2_4 = self.bn2(conv2_4)\n",
        "        relu2_4 = self.relu2(bn2_4)\n",
        "\n",
        "        # pool 1\n",
        "        pool1 = self.pool1(relu2_4)\n",
        "\n",
        "        # conv 3\n",
        "        conv3_1 = self.conv3(pool1)\n",
        "        bn3_1 = self.bn3(conv3_1)\n",
        "        relu3_1 = self.relu3(bn3_1)\n",
        "\n",
        "        conv3_2 = self.conv3_2(relu3_1)\n",
        "        bn3_2 = self.bn3(conv3_2)\n",
        "        relu3_2 = self.relu3(bn3_2)\n",
        "\n",
        "        conv3_3 = self.conv3_2(relu3_2)\n",
        "        bn3_3 = self.bn3(conv3_3)\n",
        "        relu3_3 = self.relu3(bn3_3)\n",
        "\n",
        "        conv3_4 = self.conv3_2(relu3_3)\n",
        "        bn3_4 = self.bn3(conv3_4)\n",
        "        relu3_4 = self.relu3(bn3_4)\n",
        "\n",
        "        # pool 2\n",
        "        pool2 = self.pool2(relu3_4)\n",
        "\n",
        "        # conv 4\n",
        "        conv4_1 = self.conv4(pool2)\n",
        "        bn4_1 = self.bn4(conv4_1)\n",
        "        relu4_1 = self.relu4(bn4_1)\n",
        "\n",
        "        conv4_2 = self.conv4_2(relu4_1)\n",
        "        bn4_2 = self.bn4(conv4_2)\n",
        "        relu4_2 = self.relu4(bn4_2)\n",
        "\n",
        "        conv4_3 = self.conv4_2(relu4_2)\n",
        "        bn4_3 = self.bn4(conv4_3)\n",
        "        relu4_3 = self.relu4(bn4_3)\n",
        "\n",
        "        conv4_4 = self.conv4_2(relu4_3)\n",
        "        bn4_4 = self.bn4(conv4_4)\n",
        "        relu4_4 = self.relu4(bn4_4)\n",
        "\n",
        "        # pool 3\n",
        "        pool3 = self.pool3(relu4_4)\n",
        "\n",
        "        # conv 5\n",
        "        conv5_1 = self.conv5(pool3)\n",
        "        bn5_1 = self.bn5(conv5_1)\n",
        "        relu5_1 = self.relu5(bn5_1)\n",
        "\n",
        "        conv5_2 = self.conv5_2(relu5_1)\n",
        "        bn5_2 = self.bn5(conv5_2)\n",
        "        relu5_2 = self.relu5(bn5_2)\n",
        "\n",
        "        conv5_3 = self.conv5_2(relu5_2)\n",
        "        bn5_3 = self.bn5(conv5_3)\n",
        "        relu5_3 = self.relu5(bn5_3)\n",
        "\n",
        "        conv5_4 = self.conv5_2(relu5_3)\n",
        "        bn5_4 = self.bn5(conv5_4)\n",
        "        relu5_4 = self.relu5(bn5_4)\n",
        "\n",
        "        # pool 4\n",
        "        pool4 = self.pool4(relu5_4)\n",
        "\n",
        "        # fc\n",
        "        flatten = self.flatten(pool4)\n",
        "        fc = self.fc(flatten)\n",
        "\n",
        "        return(fc)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "metadata": {
        "id": "YeqmiNTUUfhM"
      },
      "outputs": [],
      "source": [
        "# DataLoader.datasets"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l_GDS8aMyxlJ"
      },
      "source": [
        "## Train model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 63,
      "metadata": {
        "id": "CgXu7x7bzIJr"
      },
      "outputs": [],
      "source": [
        "def train_one_epoch(model: nn.Module, optim: torch.optim.Optimizer,\n",
        "                    dataloader: DataLoader, loss_fn):\n",
        "\n",
        "    num_samples = len(dataloader.dataset)\n",
        "    num_batches = len(dataloader)\n",
        "    running_corrects = 0\n",
        "    running_loss = 0.0\n",
        "\n",
        "    model.train()\n",
        "\n",
        "    for batch_indx, (inputs, targets) in enumerate(dataloader): # Get a batch of Data\n",
        "        inputs = inputs.to(device)\n",
        "        # target_tensor = torch.tensor([targets]).to(device)\n",
        "        targets = targets.to(device)\n",
        "\n",
        "        outputs = model(inputs) # Forward Pass\n",
        "        loss = loss_fn(outputs, targets) # Compute Loss\n",
        "\n",
        "        loss.backward() # Compute Gradients\n",
        "        optim.step() # Update parameters\n",
        "        optim.zero_grad() # zero the parameter's gradients\n",
        "\n",
        "        _, preds = torch.max(outputs, dim=1)\n",
        "        running_corrects += torch.sum(preds == targets).cpu()\n",
        "        running_loss += loss.item()\n",
        "\n",
        "        # print(f\"< TRAIN >--< {batch_indx} >-------------------------------\")\n",
        "        # print(f\"out:\\n{outputs}\")\n",
        "        # print(f\"tar:\\n{target_tensor}\")\n",
        "        # print(f\"running_corrects:\\n{running_corrects}\")\n",
        "        # print(f\"running_loss:\\n{running_loss}\")\n",
        "\n",
        "    epoch_acc = (running_corrects / num_samples) * 100\n",
        "    print(running_corrects, num_samples)\n",
        "    epoch_loss = (running_loss / num_batches)\n",
        "\n",
        "    return epoch_acc, epoch_loss"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C9F2sjhX2e9t"
      },
      "source": [
        "## Test model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 64,
      "metadata": {
        "id": "pPiBHmCp0jyC"
      },
      "outputs": [],
      "source": [
        "def test_model(model: nn.Module,\n",
        "               dataloader: DataLoader, loss_fn):\n",
        "\n",
        "    num_samples = len(dataloader.dataset)\n",
        "    num_batches = len(dataloader)\n",
        "    running_corrects = 0\n",
        "    running_loss = 0.0\n",
        "\n",
        "    # we call `model.eval()` to set dropout and batch normalization layers to evaluation mode before running inference.\n",
        "    model.eval()\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch_indx, (inputs, targets) in enumerate(dataloader): # Get a batch of Data\n",
        "            inputs = inputs.to(device)\n",
        "            # target_tensor = torch.tensor([targets]).to(device)\n",
        "            targets = targets.to(device)\n",
        "\n",
        "            outputs = model(inputs) # Forward Pass\n",
        "            loss = loss_fn(outputs, targets) # Compute Loss\n",
        "\n",
        "            _, preds = torch.max(outputs, 1) #\n",
        "            running_corrects += torch.sum(preds == targets).cpu()\n",
        "            running_loss += loss.item()\n",
        "\n",
        "            # print(f\"< TEST >--< {batch_indx} >-------------------------------\")\n",
        "            # print(f\"out:\\n{outputs}\")\n",
        "            # print(f\"tar:\\n{target_tensor}\")\n",
        "            # print(f\"running_corrects:\\n{running_corrects}\")\n",
        "            # print(f\"running_loss:\\n{running_loss}\")\n",
        "\n",
        "    test_acc = (running_corrects / num_samples) * 100\n",
        "    test_loss = (running_loss / num_batches)\n",
        "\n",
        "    return test_acc, test_loss"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pjWz5rRn2jlp"
      },
      "source": [
        "## Evaluate model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 69,
      "metadata": {
        "id": "bAexh8NL1SOm"
      },
      "outputs": [],
      "source": [
        "def evaluate():\n",
        "    num_epochs = 10\n",
        "    learning_rate = 0.005\n",
        "\n",
        "    full_dataloaders = {\n",
        "        'train': A_train_dl,\n",
        "        'test': A_test_dl\n",
        "    }\n",
        "\n",
        "    model = CNN()\n",
        "    model = model.to(device)\n",
        "\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
        "    cross_entropy = nn.CrossEntropyLoss()\n",
        "\n",
        "    acc_history = {'train': [], 'test': []}\n",
        "    loss_history = {'train': [], 'test': []}\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        train_acc, train_loss = train_one_epoch(model=model, optim=optimizer, dataloader=full_dataloaders['train'], loss_fn=cross_entropy)\n",
        "        test_acc, test_loss = test_model(model=model, dataloader=full_dataloaders['test'], loss_fn=cross_entropy)\n",
        "\n",
        "        acc_history['train'].append(train_acc)\n",
        "        acc_history['test'].append(test_acc)\n",
        "        loss_history['train'].append(train_loss)\n",
        "        loss_history['test'].append(test_loss)\n",
        "\n",
        "        print(f\"---------< epoch: {epoch} >---------\")\n",
        "        print(f\"acc_history:\\n{acc_history}\")\n",
        "        print(f\"loss_history:\\n{loss_history}\")\n",
        "\n",
        "    custom_plot_training_stats(acc_history, loss_history, ['train', 'test'], title='demp', dir='demo_plots')\n",
        "\n",
        "    return (acc_history, loss_history)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3tgjkkym8zzJ"
      },
      "outputs": [],
      "source": [
        "acc_history, loss_history = evaluate()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 67,
      "metadata": {
        "id": "k5rM3FZaB4j_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "df47bd68-b15e-44d6-919e-d0c16c2943e1"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'train': [7.449863910675049], 'test': [4.38879629281851]}"
            ]
          },
          "metadata": {},
          "execution_count": 67
        }
      ],
      "source": [
        "loss_history"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 68,
      "metadata": {
        "id": "dFWd-qTJrAEo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "148e23c2-13d0-4595-a905-e1c5fe5b211a"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'train': [tensor(3.2705)], 'test': [tensor(4.8114)]}"
            ]
          },
          "metadata": {},
          "execution_count": 68
        }
      ],
      "source": [
        "acc_history"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "6VoGLQNV0AOX",
        "cvgSb4Y8TDd5",
        "8X58_xpiTDd8",
        "C9F2sjhX2e9t"
      ],
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}